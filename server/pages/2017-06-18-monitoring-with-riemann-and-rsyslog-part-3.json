{"pageProps":{"source":{"compiledSource":"var c=Object.defineProperty,h=Object.defineProperties;var d=Object.getOwnPropertyDescriptors;var a=Object.getOwnPropertySymbols;var r=Object.prototype.hasOwnProperty,n=Object.prototype.propertyIsEnumerable;var l=(e,t,o)=>t in e?c(e,t,{enumerable:!0,configurable:!0,writable:!0,value:o}):e[t]=o,i=(e,t)=>{for(var o in t||(t={}))r.call(t,o)&&l(e,o,t[o]);if(a)for(var o of a(t))n.call(t,o)&&l(e,o,t[o]);return e},p=(e,t)=>h(e,d(t));var m=(e,t)=>{var o={};for(var s in e)r.call(e,s)&&t.indexOf(s)<0&&(o[s]=e[s]);if(e!=null&&a)for(var s of a(e))t.indexOf(s)<0&&n.call(e,s)&&(o[s]=e[s]);return o};const layoutProps={},MDXLayout=\"wrapper\";function MDXContent(o){var s=o,{components:e}=s,t=m(s,[\"components\"]);return mdx(MDXLayout,p(i(i({},layoutProps),t),{components:e,mdxType:\"MDXLayout\"}),mdx(\"p\",null,`Back in December I said I was interested in replacing Logstash with Rsyslog\n`,\"[https://io.made.com/blog/rek-it/]\",`, but that we needed a Riemann module to cover some of\nour existing functionality. Specifically we send metrics to Riemann from Logstash for\nthree reasons:`),mdx(\"ol\",null,mdx(\"li\",{parentName:\"ol\"},`We send internal metrics from Logstash to monitor how events flow through our log\npipeline.`),mdx(\"li\",{parentName:\"ol\"},`We forward all ERROR and CRITICAL logs to Riemann, which performs roll-up and\nthrottling. Errors are forwarded to Slack, and Criticals are sent to Pagerduty.`),mdx(\"li\",{parentName:\"ol\"},\"We allow developers to send application metrics in their structured log.\")),mdx(\"p\",null,`After some leisurely hacking over the last few days, I've got a Riemann module that\nshould cover all of our needs. We should be able to replace the internal metrics with\nimpstats `,\"[https://io.made.com/blog/monitoring-with-riemann-and-rsyslog-part-1/]\",` as\ndiscussed in part one of this series, so we won't touch on that here.`),mdx(\"p\",null,`As before I've created a docker-compose playground on GitHub. This time the code is\nunder the /custom-metrics directory.`),mdx(\"p\",null,\"Let's see how ERRORs and CRITICALs can be sent to Riemann.\"),mdx(\"p\",null,`We strongly encourage our developers to use structured logging in their applications;\nrather than simply logging text, we log JSON objects. This allows us to add more context\ninto our logs, which makes it simpler to aggregate and search in Kibana.`),mdx(\"p\",null,\"A typical structured log might look like this:\"),mdx(\"p\",null,`{ \"@timestamp\": \"2017-06-02T13:17:29.594Z\", \"@version\": \"1\", \"@message\": \"Incoming\nrequest: GET /groups/GB/cat-16631\", \"@fields\": { \"request_path\": \"/groups/GB/cat-16631\",\n\"request_method\": \"GET\", \"requestid\": \"da11e322-fbba-4f7a-a638-621da3888017\",`),mdx(\"pre\",null,mdx(\"code\",i({parentName:\"pre\"},{}),`  \"levelno\": 20,\n  \"levelname\": \"INFO\",\n\n  \"pathname\": \"./app.py\",\n  \"filename\": \"app.py\",\n  \"lineno\": 119,\n\n  \"group_id\": \"cat-16631\",\n  \"name\": \"availability-api\",\n  \"country\": \"GB\"\n}\n`)),mdx(\"p\",null,\"}\"),mdx(\"p\",null,`As well as the obvious timestamp and textual message fields, we also include some\ninformation about the request we're handling, including a correlation id, some debugging\ninformation so we can work out where the log was written, and some data from the\napplication.`),mdx(\"p\",null,`We also get logs from the Systemd Journal, and from third party and legacy applications,\nso firstly we want to normalise the logs into a common format. As before, we're going to\nuse mmnormalize for this task. Our mmnormalize rulebase\n`,\"[https://github.com/bobthemighty/rek-stack-demos/blob/master/rek/custom-json-metrics/rsyslog/rsyslog-http.rb]\",`\nhandles 3 kinds of logs: http access logs from nginx, structured json logs from our\nPython API, and log4j style logs from our processing app.`),mdx(\"p\",null,`Firstly, we use mmnormalize to parse these different log formats into a json structure,\nwith a tag so we know what kind of data we have. We're going to set a severity field on\nthe json according to some simple rules.`),mdx(\"p\",null,`For http logs, we'll use INFO as our severity level if we have a status code < 400,\notherwise ERROR. For plain text and json logs, we'll convert the log4j log-level name\ninto a syslog-severity\n`,\"[http://www.kiwisyslog.com/help/syslog/index.html?protocol_levels.htm]\",\".\"),mdx(\"ul\",null,mdx(\"li\",{parentName:\"ul\"},\"INFO -> info (6)\"),mdx(\"li\",{parentName:\"ul\"},\"WARN -> warn (4)\"),mdx(\"li\",{parentName:\"ul\"},\"ERROR -> error (3)\"),mdx(\"li\",{parentName:\"ul\"},\"CRITICAL -> critical (2)\"),mdx(\"li\",{parentName:\"ul\"},\"FATAL -> emergency (0)\")),mdx(\"p\",null,'(action type=\"mmnormalize\" rulebase=\"/etc/rsyslog/rules.rb\")'),mdx(\"h1\",null,\"If this is an http log, use the status code\"),mdx(\"p\",null,`if\n($!event.tags contains \"http\" and $!status >= 400) then { set $!severity = 3; set\n$!body!@fields!levelname\n= \"error\";`),mdx(\"h1\",null),mdx(\"h1\",null,\"If we have a json log with a level name, map it back to a severity\"),mdx(\"p\",null,`} else if ($!body!@fields!levelname != \"\") then { set $!severity =\ncnum(lookup(\"log4j_level_to_severity\", $!body!@fields!levelname)); }`),mdx(\"p\",null,\"Now that we've got a normalised log level, we can use it to forward errors to Riemann:\"),mdx(\"p\",null,\"if ($syslogseverity <= 3) then {\"),mdx(\"p\",null,'(action type=\"omriemann\" server=\"riemann\" prefix=\"errors\" description=\"!msg\") }'),mdx(\"p\",null,`By default omriemann uses the $programname as the \"service\" key in the Riemann message.\nThe prefix configuration setting prepends a fixed value to the service. When we send\nerrors through this configuration, we should see new metrics arriving in Riemann that\nlook like this:`),mdx(\"p\",null,'{:service \"errors/my-application\" :metric 1 :description \"a terrible thing occurred\" }'),mdx(\"p\",null,`With a sprinkling of Riemann magic, we can set up Slack and Email notifications with an\nhourly summary of errors across our applications.`),mdx(\"p\",null,`Several of our applications are sending metrics to Riemann via the logging pipeline.\nWe're moving away from this technique because it creates a dependency between our ELK\nstack and our metrics: when Logstash stops stashing, we lose monitoring as well as logs.\nDespite the flaws of this practice, it's simple to explain to developers, and simple for\nQA to verify, since it works by writing json to stdout.`),mdx(\"p\",null,\"A typical metric log might look like this:\"),mdx(\"p\",null,`{ \"@message\": \"Request completed: 200 - 0.003793478012084961ms.\", \"@timestamp\":\n\"2017-06-02T06:24:46.517Z\", \"@fields\": { \"requestid\":\n\"bfc153b2-a014-4a52-9090-46a1e3bf80da\", \"levelname\": \"INFO\",`),mdx(\"pre\",null,mdx(\"code\",i({parentName:\"pre\"},{}),`  \"_riemann_metric\": {\n    \"metric\": 0.003793478012084961,\n    \"ttl\": 60,\n    \"service\": \"availability.api.single_product.response_time\",\n    \"state\": \"ok\",\n    \"attributes\": {\n      \"status_code\": 200\n    }\n  }\n\n},\n\"@version\": \"1\",\n`)),mdx(\"p\",null,\"}\"),mdx(\"p\",null,`Here we have the same basic structure as our previous structured log, but with the\ninclusion of a new field `,\"_\",`riemann_metric. This metric represents the response time for\na single invocation of an API end point, and it includes the status code. We want to\npass this structure to Riemann.`),mdx(\"p\",null,\"if($!\",\"_\",'riemann_metric != \"\") then {'),mdx(\"p\",null,'action(type=\"omriemann\" server=\"riemann\" subtree=\"!body!@fields!',\"_\",`riemann_metric\"\nmode=\"single\") }`),mdx(\"p\",null,`As in previous articles, we're calling omriemann with a subtree that contains our metric\ndata. This time, however, we're using mode=single. This tells omriemann that every\nmessage contains a single metric, and that the fields of the json subtree should be\nmapped to the fields of the Riemann message. This is different to our previous examples,\nwhere we expect that each field of the subtree is a different metric.`),mdx(\"p\",null,`As before, I've included a dummy web application that returns a hard-coded status code\nat each endpoint. This time, you can include a sleep time in milliseconds: curl\nlocalhost/404?sleep=100.`),mdx(\"p\",null,`This will result in a metric log being sent to Rsyslog and through to Riemann so that we\ncan alert and report on it.`),mdx(\"p\",null,`Now that I can reproduce our Logstash/Riemann integration with Rsyslog, I can begin the\nprocess of building a new REK stack.`))}MDXContent.isMDXComponent=!0;\n","scope":{}},"date":"2017-06-18","title":"Monitoring with Riemann and rsyslog part 3","layout":"post","author":"Bob","tags":["riemann","monitoring","elk"]},"__N_SSG":true}